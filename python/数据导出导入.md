# 数据导出导入

```python
import pandas as pd
from sqlalchemy import create_engine

# 数据库连接配置
engine = create_engine('mysql+pymysql://user:pass@localhost/db')

# 场景1：分块读取SQL并写入CSV（适合超大数据导出）
query = "SELECT * FROM billion_rows_table"
chunk_size = 50000  # 每块5万条记录

with open('large_export.csv', 'w') as f:
    # 首次写入带表头
    first_chunk = True
    for chunk in pd.read_sql_query(query, engine, chunksize=chunk_size):
        chunk.to_csv(f, header=first_chunk, index=False)
        first_chunk = False
        print(f"已导出 {len(chunk)} 条记录")

# 场景2：分块读取CSV并写入数据库（适合大数据导入）
def chunk_importer(csv_path):
    for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):
        chunk.to_sql('target_table', 
                    engine, 
                    if_exists='append' if i>0 else 'replace',
                    index=False)
        print(f"已导入第 {i+1} 批数据，共 {len(chunk)} 条")

chunk_importer('huge_data.csv')
```

```python
import pandas as pd
from sqlalchemy import create_engine

# 数据库连接配置
engine = create_engine('mysql+pymysql://user:pass@localhost/db', connect_args={'charset': 'utf8mb4'})

# 场景1：分块读取SQL并写入CSV（适合超大数据导出）
query = "SELECT * FROM billion_rows_table"
chunk_size = 50000  # 每块5万条记录

with open('large_export.csv', 'w', encoding='utf-8') as f:
    # 首次写入带表头
    first_chunk = True
    for chunk in pd.read_sql_query(query, engine, chunksize=chunk_size):
        chunk.to_csv(f, header=first_chunk, index=False)
        first_chunk = False
        print(f"已导出 {len(chunk)} 条记录")
```

```python
import pandas as pd
from sqlalchemy import create_engine

# 数据库连接配置
engine = create_engine('mysql+pymysql://user:pass@localhost/db')

# 场景1：分块读取SQL并写入CSV（适合超大数据导出）
query = "SELECT * FROM billion_rows_table"
chunk_size = 50000  # 每块5万条记录

with open('large_export.csv', 'w') as f:
    # 首次写入带表头
    first_chunk = True
    for chunk in pd.read_sql_query(query, engine, chunksize=chunk_size):
        chunk.to_csv(f, header=first_chunk, index=False)
        first_chunk = False
        print(f"已导出 {len(chunk)} 条记录")

# 场景2：分块读取CSV并写入数据库（适合大数据导入）
def chunk_importer(csv_path):
    for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):
        chunk.to_sql('target_table', 
                    engine, 
                    if_exists='append' if i>0 else 'replace',
                    index=False)
        print(f"已导入第 {i+1} 批数据，共 {len(chunk)} 条")

chunk_importer('huge_data.csv')
```

## 导出

### 一、基础环境配置
1. **安装必备库**  
   ```bash
   pip install pandas mysql-connector-python
   ```
   MySQL 官方驱动和数据处理库组合效率最高

2. **连接数据库配置**  
   ```python
   conn = mysql.connector.connect(
       host='localhost',
       user='your_username',
       password='your_password',
       database='your_database'
   )
   ```
   需替换为实际数据库凭证

### 二、核心导出方法
1. **全表导出为CSV**  
   ```python
   df = pd.read_sql("SELECT * FROM table_name", conn)
   df.to_csv('output.csv', index=False)
   ```
   Pandas 的 `read_sql` 直接加载查询结果到 DataFrame

2. **分块导出大数据**  
   ```python
   chunksize = 100000
   for chunk in pd.read_sql_query("SELECT * FROM large_table", conn, chunksize=chunksize):
       chunk.to_csv('output.csv', mode='a', header=False)
   ```
   百万级数据建议分块处理避免内存溢出

### 三、高级功能扩展
1. **导出表结构到Excel**  
   ```python
   tables = pd.read_sql("SHOW TABLES", conn)
   with pd.ExcelWriter('schema.xlsx') as writer:
       tables.to_excel(writer, sheet_name='Tables')
   ```
   可结合 `INFORMATION_SCHEMA` 获取字段详情

2. **JSON格式导出**  
   ```python
   df.to_json('data.json', orient='records')
   ```
   适合API接口数据交换场景

### 四、注意事项
- 连接用完需手动关闭 `conn.close()`
- 导出路径需有写入权限
- 大数据导出建议禁用索引 `index=False`

## Pandas导入MySQL数据完整指南

## 一、基础环境准备

### 1. 安装依赖库

`pip install pandas sqlalchemy pymysql`

- 推荐使用SQLAlchemy作为ORM工具，兼容性更好

### 2. 数据库连接配置

`from sqlalchemy import create_engine engine = create_engine('mysql+pymysql://user:password@host:port/database?charset=utf8mb4')`

- 需替换实际数据库参数
- 建议使用utf8mb4字符集支持完整Unicode

## 二、核心导入方法

### 1. CSV文件导入

`import pandas as pd df = pd.read_csv('data.csv') df.to_sql('target_table', engine, if_exists='append', index=False)`

- `if_exists`参数说明：
  - `append`：追加数据
  - `replace`：替换表数据
  - `fail`：表存在时报错

### 2. Excel文件导入

`df = pd.read_excel('data.xlsx', sheet_name='Sheet1') df.to_sql('table_name', engine, chunksize=10000)`

- 大数据量建议设置chunksize分块写入

## 三、高级功能扩展

### 1. 动态建表导入

`df = pd.read_csv('data.csv') dtype_map = {'id':'INT', 'name':'VARCHAR(255)'} df.to_sql('new_table', engine, dtype=dtype_map)`

- 自动创建不存在的表结构
- 需手动指定字段SQL类型

### 2. 增量数据导入

`existing_ids = pd.read_sql('SELECT id FROM table', engine)['id'].values new_data = df[~df['id'].isin(existing_ids)] new_data.to_sql('table', engine, if_exists='append')`

- 通过ID比对实现增量插入
- 避免重复数据问题

## 四、注意事项

1. **性能优化**：

   `engine.execution_options(autocommit=False)`

   - 大数据导入时关闭自动提交提升性能

2. **类型匹配**：

   - 特别注意日期时间类型的格式转换
   - 字符串字段建议明确指定长度

3. **路径处理**：

   `df = pd.read_csv(r'C:\data.csv')`

   - Windows路径建议使用原始字符串
