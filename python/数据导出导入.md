# 数据导出导入

##

```python
import pandas as pd
from sqlalchemy import create_engine, text
from contextlib import contextmanager

# 数据源配置字典
DATABASES = {
    'test': 'mysql+pymysql://user:pass@local:3306/test'
}

@contextmanager
def switch_database(db_name):
    engine = create_engine(DATABASES[db_name], connect_args={'charset': 'utf8mb4'})
    try:
        yield engine.connect()
    finally:
        engine.dispose()

# 使用示例
with switch_database('test') as conn:
    # 场景1：分块读取SQL并写入CSV（适合超大数据导出）
    query = r"SELECT * FROM test"
    chunk_size = 50000  # 每块5万条记录

    with open('large_export.csv', 'w', encoding='utf-8') as f:
        # 首次写入带表头
        first_chunk = True
        for chunk in pd.read_sql_query(query, conn, chunksize=chunk_size):
            chunk.to_csv(f, header=first_chunk, index=False)
            first_chunk = False
            print(f"已导出 {len(chunk)} 条记录")

    # 读取 large_export.csv 文件
    df = pd.read_csv('large_export.csv')
    # 遍历 平台账户ID 列
    for index, row in df.iterrows():
        user_password_ids = row['平台账户ID']
        # 判断 user_password_ids 不为空，解析格式为 xxx,xxx,xxx
        if pd.isna(user_password_ids):
            continue
        # 解析格式为 xxx,xxx,xxx
        user_password_ids_list = user_password_ids.split(',')
        # 打印结果
        # print(f"ID: {row['ID']}, 平台账户ID列表: {user_password_ids_list}")
        # 查询数据库获取平台账户信息 转义 in 语句
        if len(user_password_ids_list) > 0:
            query = text(r"SELECT COALESCE(A, B), CONCAT_WS(',', C, D) AS 平台类型 FROM test2 WHERE name IN (:param)")
            params = {'param': user_password_ids_list}
            df2 = pd.read_sql_query(query, conn, params=params)
            # 将 df2 结果组装到 row 中
            row['平台账户信息'] = df2.to_dict(orient='records')
            # row 设置组装回 df
            df.loc[index, '平台账户信息'] = row['平台账户信息']

    # 打印结果
    print(df)
    # 写入文件
    df.to_csv('output.csv', index=False)
````

##

```python
import pandas as pd
from sqlalchemy import create_engine

# 数据库连接配置
engine = create_engine('mysql+pymysql://user:pass@localhost/db')

# 场景1：分块读取SQL并写入CSV（适合超大数据导出）
query = "SELECT * FROM billion_rows_table"
chunk_size = 50000  # 每块5万条记录

with open('large_export.csv', 'w') as f:
    # 首次写入带表头
    first_chunk = True
    for chunk in pd.read_sql_query(query, engine, chunksize=chunk_size):
        chunk.to_csv(f, header=first_chunk, index=False)
        first_chunk = False
        print(f"已导出 {len(chunk)} 条记录")

# 场景2：分块读取CSV并写入数据库（适合大数据导入）
def chunk_importer(csv_path):
    for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):
        chunk.to_sql('target_table', 
                    engine, 
                    if_exists='append' if i>0 else 'replace',
                    index=False)
        print(f"已导入第 {i+1} 批数据，共 {len(chunk)} 条")

chunk_importer('huge_data.csv')
```

```python
import pandas as pd
from sqlalchemy import create_engine
from contextlib import contextmanager

# 数据源配置字典
DATABASES = {
    'db1': 'mysql+pymysql://user:pass@localhost/db1',
    'db2': 'mysql+pymysql://user:pass@localhost/db2'
}

@contextmanager
def switch_database(db_name):
    engine = create_engine(DATABASES[db_name], connect_args={'charset': 'utf8mb4'})
    try:
        yield engine.connect()
    finally:
        engine.dispose()

# 使用示例
with switch_database('yl_shop') as conn:
    # 场景1：分块读取SQL并写入CSV（适合超大数据导出）
    query = "SELECT * FROM billion_rows_table"
    chunk_size = 50000  # 每块5万条记录

    with open('large_export.csv', 'w', encoding='utf-8') as f:
        # 首次写入带表头
        first_chunk = True
        for chunk in pd.read_sql_query(query, conn, chunksize=chunk_size):
            chunk.to_csv(f, header=first_chunk, index=False)
            first_chunk = False
            print(f"已导出 {len(chunk)} 条记录")
```

```python
import pandas as pd
from sqlalchemy import create_engine

# 数据库连接配置
engine = create_engine('mysql+pymysql://user:pass@localhost/db')

# 场景1：分块读取SQL并写入CSV（适合超大数据导出）
query = "SELECT * FROM billion_rows_table"
chunk_size = 50000  # 每块5万条记录

with open('large_export.csv', 'w') as f:
    # 首次写入带表头
    first_chunk = True
    for chunk in pd.read_sql_query(query, engine, chunksize=chunk_size):
        chunk.to_csv(f, header=first_chunk, index=False)
        first_chunk = False
        print(f"已导出 {len(chunk)} 条记录")

# 场景2：分块读取CSV并写入数据库（适合大数据导入）
def chunk_importer(csv_path):
    for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):
        chunk.to_sql('target_table', 
                    engine, 
                    if_exists='append' if i>0 else 'replace',
                    index=False)
        print(f"已导入第 {i+1} 批数据，共 {len(chunk)} 条")

chunk_importer('huge_data.csv')
```

## 导出

### 一、基础环境配置
1. **安装必备库**  
   ```bash
   pip install pandas mysql-connector-python
   ```
   MySQL 官方驱动和数据处理库组合效率最高

2. **连接数据库配置**  
   ```python
   conn = mysql.connector.connect(
       host='localhost',
       user='your_username',
       password='your_password',
       database='your_database'
   )
   ```
   需替换为实际数据库凭证

### 二、核心导出方法
1. **全表导出为CSV**  
   ```python
   df = pd.read_sql("SELECT * FROM table_name", conn)
   df.to_csv('output.csv', index=False)
   ```
   Pandas 的 `read_sql` 直接加载查询结果到 DataFrame

2. **分块导出大数据**  
   ```python
   chunksize = 100000
   for chunk in pd.read_sql_query("SELECT * FROM large_table", conn, chunksize=chunksize):
       chunk.to_csv('output.csv', mode='a', header=False)
   ```
   百万级数据建议分块处理避免内存溢出

### 三、高级功能扩展
1. **导出表结构到Excel**  
   ```python
   tables = pd.read_sql("SHOW TABLES", conn)
   with pd.ExcelWriter('schema.xlsx') as writer:
       tables.to_excel(writer, sheet_name='Tables')
   ```
   可结合 `INFORMATION_SCHEMA` 获取字段详情

2. **JSON格式导出**  
   ```python
   df.to_json('data.json', orient='records')
   ```
   适合API接口数据交换场景

### 四、注意事项
- 连接用完需手动关闭 `conn.close()`
- 导出路径需有写入权限
- 大数据导出建议禁用索引 `index=False`

## Pandas导入MySQL数据完整指南

## 一、基础环境准备

### 1. 安装依赖库

`pip install pandas sqlalchemy pymysql`

- 推荐使用SQLAlchemy作为ORM工具，兼容性更好

### 2. 数据库连接配置

`from sqlalchemy import create_engine engine = create_engine('mysql+pymysql://user:password@host:port/database?charset=utf8mb4')`

- 需替换实际数据库参数
- 建议使用utf8mb4字符集支持完整Unicode

## 二、核心导入方法

### 1. CSV文件导入

`import pandas as pd df = pd.read_csv('data.csv') df.to_sql('target_table', engine, if_exists='append', index=False)`

- `if_exists`参数说明：
  - `append`：追加数据
  - `replace`：替换表数据
  - `fail`：表存在时报错

### 2. Excel文件导入

```python
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
df.to_sql('table_name', engine, chunksize=10000)
```

- 大数据量建议设置chunksize分块写入

## 三、高级功能扩展

### 1. 动态建表导入

```python
df = pd.read_csv('data.csv')
dtype_map = {'id':'INT', 'name':'VARCHAR(255)'}
df.to_sql('new_table', engine, dtype=dtype_map)
```

- 自动创建不存在的表结构
- 需手动指定字段SQL类型

### 2. 增量数据导入

```python
existing_ids = pd.read_sql('SELECT id FROM table', engine)['id'].values
new_data = df[~df['id'].isin(existing_ids)]
new_data.to_sql('table', engine, if_exists='append')
```

- 通过ID比对实现增量插入
- 避免重复数据问题

## 四、注意事项

1. **性能优化**：

   `engine.execution_options(autocommit=False)`

   - 大数据导入时关闭自动提交提升性能

2. **类型匹配**：

   - 特别注意日期时间类型的格式转换
   - 字符串字段建议明确指定长度

3. **路径处理**：

   `df = pd.read_csv(r'C:\data.csv')`

   - Windows路径建议使用原始字符串
  
## 分批次导出

```python
import pandas as pd
from sqlalchemy import create_engine, text
from contextlib import contextmanager

# 数据源配置字典
DATABASES = {
    'yl_shop': 'mysql+pymysql://xxxxxx/test',
}

@contextmanager
def switch_database(db_name):
    engine = create_engine(DATABASES[db_name], connect_args={'charset': 'utf8mb4'})
    try:
        yield engine.connect()
    finally:
        engine.dispose()

# 从 excel 中读取数据
df = pd.read_excel('test.xlsx', sheet_name='企业id')
# 收集所有团队ID并设置分批次大小
team_ids = df['企业id'].tolist()
chunksize = 500  # 可根据实际情况调整批次大小
team_info = {}

# 分批次处理团队ID
for i in range(0, len(team_ids), chunksize):
    batch_ids = team_ids[i:i+chunksize]
    print(f'处理批次 {i//chunksize+1}/{(len(team_ids)-1)//chunksize+1}, 团队数量: {len(batch_ids)}')
    
    # 批量查询当前批次团队的最新环境信息
    with switch_database('test') as conn:
        query = text("""
            WITH ranked_logins AS (
                SELECT 
                    login_id, 
                    company_id, 
                    last_login_at,
                    ROW_NUMBER() OVER (PARTITION BY company_id ORDER BY last_login_at DESC) as rn
                FROM login_logs 
                WHERE company_id IN :team_ids
            )
            SELECT account_id, company_id, last_login_at 
            FROM ranked_accounts 
            WHERE rn = 1
        """)
        result = conn.execute(query, {'team_ids': tuple(batch_ids)})
        # 将批次结果合并到总结果字典
        batch_info = {row.company_id: (row.login_id, row.last_login_at) for row in result}
        team_info.update(batch_info)

# 填充数据到DataFrame
for index, row in df.iterrows():
    team_id = row['企业id']
    print(team_id)
    if team_id in team_info:
        login_id, last_login_at = team_info[team_id]
        df.loc[index, 'ID'] = login_id
        df.loc[index, '最后登录时间'] = last_login_at
    else:
        df.loc[index, 'ID'] = None
        df.loc[index, '最后登录时间'] = None

# 将结果保存到新的 excel 文件中
df.to_excel('export_shop_company3.xlsx', index=False, sheet_name='所有环境_团队id')
```

```python
import pandas as pd
from sqlalchemy import create_engine, text
from contextlib import contextmanager

# 数据源配置字典
DATABASES = {
    'browser_logs': 'postgresql+psycopg2://xxxxxx:5432/yl_stats',
}

@contextmanager
def switch_database(db_name):
    engine = create_engine(DATABASES[db_name])
    try:
        yield engine.connect()
    finally:
        engine.dispose()

# 从 excel 中读取数据
df = pd.read_excel('test.xlsx', sheet_name='ID')
env_ids = df['ID'].tolist()

with switch_database('browser_logs') as conn:
    num = 0
    # 逐个处理环境ID
    for env_id in env_ids:
        num += 1
        if num < 37002:
            continue

        print(f'处理环境 {env_id} ({num}/{len(env_ids)})')

        # 查询当前环境的最新10条浏览记录并提取域名
        query = text("""
                SELECT
                    shop_id,
                    SPLIT_PART(url, '/', 3) AS domain  -- 提取域名
                FROM public.browser_logs
                WHERE id = :env_id AND created_at >= '2025-07-01 00:00:00' LIMIT 10
        """)
        try:
            result = conn.execute(query, {'env_id': env_id})

            # 处理结果：去重并保留最新10个域名
            domains = []
            seen = set()
            for row in result:
                domain = row.domain
                if domain not in seen:
                    domains.append(domain)
                    seen.add(domain)
                    if len(domains) >= 10:
                        break

            # 更新DataFrame中当前环境的域名信息
            idx = df.index[df['ID'] == env_id].tolist()[0]
            for i in range(10):
                col_name = f'domain_{i+1}'
                df.at[idx, col_name] = domains[i] if i < len(domains) else ''

            # 即时保存当前处理结果
            if num%1000 == 0:
                df.to_excel('export_shop_domain.xlsx', index=False)
                print(f'环境 {env_id} 处理完成并保存文件')
        except Exception as e:
            conn.rollback()
            conn.close()
            print(f'环境 {env_id} 处理失败: {e}')
            conn = switch_database('browser_logs')
```
合并
```python
import pandas as pd
import os

def merge_excel_files(file1, file2, output_file):
    # 读取两个Excel文件
    df1 = pd.read_excel(file1)
    df2 = pd.read_excel(file2)
    
    # 行合并数据
    merged_df = pd.concat([df1, df2], ignore_index=True)
    
    # 保存合并后的结果
    merged_df.to_excel(output_file, index=False)
    print(f"成功合并文件，已保存至: {output_file}")


def merge_excel_files2(file1, file2, output_file):
    # 读取两个Excel文件
    df1 = pd.read_excel(file1)
    df2 = pd.read_excel(file2)
    
    # 按列合并并取非空值 - 使用df2的值填充df1中的空值
    # 保留两个文件的所有列，相同列名时取非空值
    merged_df = df1.combine_first(df2)
    
    # 如果需要反向优先级(用df1填充df2)，可以使用:
    # merged_df = df2.combine_first(df1)
    
    # 保存合并后的结果
    merged_df.to_excel(output_file, index=False)
    print(f"成功合并文件，已保存至: {output_file}")

if __name__ == "__main__":
    # 添加合并文件的调用
    merge_excel_files2(
        "export_shop_domain.xlsx",
        "export_shop_domain (4).xlsx",
        "export_shop_domain_new.xlsx"
    )
```
