## 各场景算法使用示例

### 1. **人名/地址匹配 - Jaro-Winkler**
```python
import textdistance

def match_names(names_list, target_name, threshold=0.85):
    """
    匹配相似的人名/地址
    threshold: Jaro-Winkler通常>0.9表示非常相似
    """
    matches = []
    
    for name in names_list:
        # 计算Jaro-Winkler相似度 (0-1之间)
        similarity = textdistance.jaro_winkler(target_name, name)
        
        if similarity >= threshold:
            matches.append({
                'name': name,
                'similarity': round(similarity, 3),
                'algorithm': 'Jaro-Winkler'
            })
    
    # 按相似度排序
    return sorted(matches, key=lambda x: x['similarity'], reverse=True)

# 示例使用
names = ["John Smith", "Jon Smith", "John Smyth", "Jane Doe", "John S. Smith"]
target = "John Smith"

results = match_names(names, target, threshold=0.8)
for r in results:
    print(f"{r['name']}: {r['similarity']}")

# 输出：
# John Smith: 1.0
# Jon Smith: 0.853
# John Smyth: 0.853
# John S. Smith: 0.827
```

### 2. **拼写纠错 - Levenshtein（加权版）**
```python
def weighted_spell_check(word, dictionary, max_distance=2):
    """
    加权编辑距离拼写纠错
    可以自定义不同编辑操作的权重
    """
    suggestions = []
    
    for correct_word in dictionary:
        # 标准Levenshtein距离
        basic_distance = textdistance.levenshtein(word, correct_word)
        
        # 带权重的Levenshtein（需要自定义函数）
        # 假设：替换权重=1，插入=1，删除=1，交换相邻字符=0.5
        # textdistance没有内置加权版，但可以组合使用
        weighted_distance = calculate_weighted_levenshtein(word, correct_word)
        
        # 或者使用Damerau-Levenshtein（包含相邻字符交换）
        damerau_dist = textdistance.damerau_levenshtein(word, correct_word)
        
        if damerau_dist <= max_distance:
            suggestions.append({
                'word': correct_word,
                'levenshtein': basic_distance,
                'damerau_levenshtein': damerau_dist,
                'weighted': weighted_distance
            })
    
    return sorted(suggestions, key=lambda x: x['damerau_levenshtein'])

# 辅助函数：自定义加权编辑距离
def calculate_weighted_levenshtein(s1, s2):
    """自定义加权编辑距离实现"""
    # 简化的加权实现（实际应用需要更复杂的动态规划）
    if s1 == s2:
        return 0
    
    # 这里使用标准Levenshtein作为示例
    # 实际应用中可以使用更复杂的权重矩阵
    return textdistance.levenshtein(s1, s2)

# 示例使用
dictionary = ["apple", "application", "appliance", "ape", "apply", "banana"]
word = "aplle"

suggestions = weighted_spell_check(word, dictionary, max_distance=2)
for s in suggestions:
    print(f"候选: {s['word']}, 编辑距离: {s['damerau_levenshtein']}")
```

### 3. **文档去重 - Jaccard/Sørensen-Dice (n-gram版)**
```python
def document_similarity(doc1, doc2, ngram_size=2, method='jaccard'):
    """
    使用n-gram计算文档相似度
    method: 'jaccard' 或 'sorensen'
    """
    # 创建n-gram集合
    def get_ngrams(text, n):
        return {text[i:i+n] for i in range(len(text)-n+1)}
    
    ngrams1 = get_ngrams(doc1.lower(), ngram_size)
    ngrams2 = get_ngrams(doc2.lower(), ngram_size)
    
    if method == 'jaccard':
        similarity = textdistance.jaccard(ngrams1, ngrams2)
    elif method == 'sorensen':
        similarity = textdistance.sorensen(ngrams1, ngrams2)
    else:
        raise ValueError("method must be 'jaccard' or 'sorensen'")
    
    return similarity

def find_duplicate_documents(documents, threshold=0.7):
    """
    批量查找重复文档
    """
    duplicates = []
    
    for i in range(len(documents)):
        for j in range(i+1, len(documents)):
            # 使用Jaccard相似度
            similarity = document_similarity(
                documents[i], 
                documents[j],
                ngram_size=3,
                method='jaccard'
            )
            
            if similarity >= threshold:
                duplicates.append({
                    'doc1_index': i,
                    'doc2_index': j,
                    'similarity': round(similarity, 3),
                    'snippet1': documents[i][:50] + "...",
                    'snippet2': documents[j][:50] + "..."
                })
    
    return sorted(duplicates, key=lambda x: x['similarity'], reverse=True)

# 示例使用
docs = [
    "The quick brown fox jumps over the lazy dog",
    "A quick brown fox jumps over the lazy dog",
    "The fast brown fox jumps over the sleepy dog",
    "完全不同的句子，不会有相似度",
    "The quick brown fox jumps over lazy dog"
]

duplicates = find_duplicate_documents(docs, threshold=0.5)
for dup in duplicates:
    print(f"文档{dup['doc1_index']} ≈ 文档{dup['doc2_index']}: {dup['similarity']}")
```

### 4. **基因序列/抄袭检测 - Smith-Waterman-Gotoh**
```python
def local_sequence_alignment(seq1, seq2, match_score=2, mismatch_penalty=-1, gap_penalty=-0.5):
    """
    局部序列比对（Smith-Waterman算法）
    用于基因序列或抄袭检测
    """
    # textdistance的smith_waterman算法
    # 注意：textdistance的smith_waterman返回相似度分数，不是对齐结果
    alignment_score = textdistance.smith_waterman(
        seq1, 
        seq2,
        gap_open=gap_penalty,
        gap_ext=gap_penalty,
        match=match_score,
        mismatch=mismatch_penalty
    )
    
    # 计算归一化相似度（基于最长序列）
    max_length = max(len(seq1), len(seq2))
    normalized_score = alignment_score / (max_length * match_score) if max_length > 0 else 0
    
    return {
        'raw_score': alignment_score,
        'normalized_score': round(normalized_score, 3),
        'match_percentage': round(alignment_score / (len(seq1) * match_score), 3) if len(seq1) > 0 else 0
    }

def detect_plagiarism(text1, text2, min_length=10):
    """
    抄袭检测示例
    将文本分成片段进行局部比对
    """
    # 将文本分成重叠片段
    def get_segments(text, segment_length=50, overlap=25):
        segments = []
        for i in range(0, len(text)-segment_length+1, overlap):
            segments.append(text[i:i+segment_length])
        return segments
    
    segments1 = get_segments(text1)
    segments2 = get_segments(text2)
    
    matches = []
    
    for i, seg1 in enumerate(segments1):
        for j, seg2 in enumerate(segments2):
            if len(seg1) >= min_length and len(seg2) >= min_length:
                result = local_sequence_alignment(seg1, seg2)
                
                if result['normalized_score'] > 0.7:  # 阈值
                    matches.append({
                        'segment1': seg1,
                        'segment2': seg2,
                        'score': result['normalized_score'],
                        'position1': i * 25,  # 估算位置
                        'position2': j * 25
                    })
    
    return matches

# 示例使用
seq1 = "GATTACA"
seq2 = "GCATGCU"
alignment = local_sequence_alignment(seq1, seq2)
print(f"序列比对分数: {alignment}")

# 文本抄袭检测
text_a = "This is a sample text for testing plagiarism detection algorithm."
text_b = "This sample text is for testing the plagiarism detection method."

plagiarism_matches = detect_plagiarism(text_a, text_b)
for match in plagiarism_matches:
    print(f"相似片段: {match['score']}")
```

### 5. **短文本相似度 - 重叠系数 + Jaccard组合**
```python
def short_text_similarity(text1, text2, use_combination=True):
    """
    短文本相似度计算
    组合使用重叠系数和Jaccard
    """
    # 分词函数（简单版）
    def tokenize(text):
        return set(text.lower().split())
    
    tokens1 = tokenize(text1)
    tokens2 = tokenize(text2)
    
    # 分别计算两种相似度
    overlap_sim = textdistance.overlap(tokens1, tokens2)  # 重叠系数
    jaccard_sim = textdistance.jaccard(tokens1, tokens2)  # Jaccard系数
    
    if use_combination:
        # 组合策略：取平均值，或根据需求加权
        # 重叠系数对包含关系敏感，Jaccard对整体相似度敏感
        combined_score = (overlap_sim + jaccard_sim) / 2
        return {
            'overlap': round(overlap_sim, 3),
            'jaccard': round(jaccard_sim, 3),
            'combined': round(combined_score, 3),
            'method': 'combined'
        }
    else:
        return {
            'overlap': round(overlap_sim, 3),
            'jaccard': round(jaccard_sim, 3),
            'method': 'individual'
        }

def find_similar_short_texts(query, text_list, threshold=0.5):
    """
    在短文本集合中查找相似文本
    """
    results = []
    
    for text in text_list:
        similarity = short_text_similarity(query, text, use_combination=True)
        
        if similarity['combined'] >= threshold:
            results.append({
                'text': text,
                'similarity': similarity['combined'],
                'details': similarity
            })
    
    return sorted(results, key=lambda x: x['similarity'], reverse=True)

# 示例使用
query = "quick brown fox"
texts = [
    "brown fox quick",
    "quick brown dog",
    "fast brown fox",
    "the quick brown fox jumps",
    "completely different"
]

similar_texts = find_similar_short_texts(query, texts, threshold=0.3)
for result in similar_texts:
    print(f"文本: {result['text'][:30]}... 相似度: {result['similarity']}")
    print(f"  细节: Overlap={result['details']['overlap']}, Jaccard={result['details']['jaccard']}")
```

### 6. **编码/哈希校验 - Hamming距离**
```python
def hamming_compare(binary_str1, binary_str2):
    """
    Hamming距离比较
    适用于等长的二进制字符串、哈希值、错误校正码
    """
    if len(binary_str1) != len(binary_str2):
        raise ValueError("Hamming距离只适用于等长字符串")
    
    # 计算Hamming距离
    distance = textdistance.hamming(binary_str1, binary_str2)
    
    # 计算相似度百分比
    length = len(binary_str1)
    similarity_percent = ((length - distance) / length) * 100 if length > 0 else 0
    
    # 错误率
    error_rate = distance / length if length > 0 else 0
    
    return {
        'distance': distance,
        'similarity_percent': round(similarity_percent, 2),
        'error_rate': round(error_rate, 4),
        'is_perfect_match': distance == 0
    }

def validate_checksum(data, checksum, checksum_func=None):
    """
    使用Hamming距离验证校验和/哈希
    """
    if checksum_func:
        calculated = checksum_func(data)
    else:
        # 简单示例：使用内置hash
        calculated = str(hash(data) % 10000).zfill(4)
    
    result = hamming_compare(str(checksum), str(calculated))
    
    # 根据Hamming距离判断有效性
    if result['distance'] == 0:
        status = "VALID"
    elif result['distance'] <= 1:  # 允许1位错误
        status = "CORRECTABLE"
    else:
        status = "INVALID"
    
    result['status'] = status
    result['expected'] = checksum
    result['calculated'] = calculated
    
    return result

# 示例使用
# 1. 直接Hamming比较
code1 = "10101010"
code2 = "10101011"
print("Hamming比较:", hamming_compare(code1, code2))

# 2. 哈希校验
data = "important data"
checksum = "1234"
validation = validate_checksum(data, checksum)
print(f"校验结果: {validation['status']}, 距离: {validation['distance']}")

# 3. DNA序列错误检测（简化版）
dna1 = "AGCTAGCT"
dna2 = "AGCTAGCA"
print("DNA序列差异:", hamming_compare(dna1, dna2))
```

### 7. **推荐系统相似度 - Cosine相似度**
```python
import math
from collections import Counter

def cosine_similarity(text1, text2, use_tfidf=False):
    """
    Cosine相似度计算
    textdistance库没有直接提供cosine，但可以基于向量计算
    """
    # 创建词频向量
    def text_to_vector(text):
        words = text.lower().split()
        return Counter(words)
    
    vector1 = text_to_vector(text1)
    vector2 = text_to_vector(text2)
    
    # 获取所有单词
    all_words = set(vector1.keys()) | set(vector2.keys())
    
    # 创建数值向量
    vec1 = [vector1.get(word, 0) for word in all_words]
    vec2 = [vector2.get(word, 0) for word in all_words]
    
    # 计算cosine相似度
    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(v * v for v in vec1))
    magnitude2 = math.sqrt(sum(v * v for v in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return dot_product / (magnitude1 * magnitude2)

def find_similar_items(user_item_vector, item_vectors, top_n=5):
    """
    基于Cosine相似度的推荐系统
    user_item_vector: 用户对物品的偏好向量
    item_vectors: 所有物品的特征向量
    """
    similarities = []
    
    for item_id, item_vector in item_vectors.items():
        # 计算cosine相似度
        similarity = cosine_similarity_vector(user_item_vector, item_vector)
        
        if similarity > 0:
            similarities.append({
                'item_id': item_id,
                'similarity': round(similarity, 3)
            })
    
    # 返回最相似的top_n个物品
    return sorted(similarities, key=lambda x: x['similarity'], reverse=True)[:top_n]

def cosine_similarity_vector(vec1, vec2):
    """向量形式的cosine相似度"""
    # 确保向量维度相同
    if len(vec1) != len(vec2):
        raise ValueError("向量维度必须相同")
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

# 示例使用
# 1. 文本Cosine相似度
text1 = "machine learning artificial intelligence"
text2 = "artificial intelligence deep learning"
print("文本Cosine相似度:", cosine_similarity(text1, text2))

# 2. 推荐系统示例
# 用户偏好向量（对5个特征的偏好程度）
user_pref = [1, 0, 3, 0, 2]

# 物品特征向量
items = {
    'item1': [2, 1, 3, 0, 1],
    'item2': [0, 0, 1, 2, 3],
    'item3': [1, 2, 2, 1, 0],
    'item4': [3, 0, 1, 0, 2],
    'item5': [0, 3, 0, 2, 1]
}

recommendations = find_similar_items(user_pref, items, top_n=3)
print("推荐物品:")
for rec in recommendations:
    print(f"  物品{rec['item_id']}: 相似度={rec['similarity']}")
```

## **textdistance 高级技巧**

### 批量处理优化
```python
from functools import lru_cache

# 使用缓存加速重复计算
@lru_cache(maxsize=1000)
def cached_similarity(text1, text2, algorithm='levenshtein'):
    """缓存相似度计算结果"""
    algorithm_func = getattr(textdistance, algorithm)
    return algorithm_func(text1, text2)

# 并行计算
from concurrent.futures import ThreadPoolExecutor

def batch_similarity(text_pairs, algorithm='jaro_winkler'):
    """批量计算相似度"""
    with ThreadPoolExecutor() as executor:
        results = list(executor.map(
            lambda pair: textdistance.jaro_winkler(pair[0], pair[1]),
            text_pairs
        ))
    return results
```

### 自定义算法参数
```python
# 自定义Jaro-Winkler参数
def custom_jaro_winkler(s1, s2, winkler_bonus=0.1, scaling_factor=0.1):
    """
    自定义参数的Jaro-Winkler
    winkler_bonus: 前缀奖励阈值
    scaling_factor: 前缀权重系数
    """
    # textdistance允许参数传递
    return textdistance.jaro_winkler(
        s1, s2,
        winkler_bonus=winkler_bonus,
        scaling_factor=scaling_factor
    )

# 自定义编辑距离权重
def custom_levenshtein(s1, s2, insert_cost=1, delete_cost=1, replace_cost=1):
    """
    自定义权重的编辑距离
    textdistance的levenshtein不支持自定义权重
    但可以使用其他库或自定义实现
    """
    # 这里使用标准Levenshtein
    return textdistance.levenshtein(s1, s2)
```

### 多算法融合策略
```python
def hybrid_similarity(s1, s2, weights=None):
    """
    多算法融合的相似度计算
    """
    if weights is None:
        weights = {
            'jaro_winkler': 0.4,
            'levenshtein': 0.3,
            'jaccard': 0.3
        }
    
    algorithms = {
        'jaro_winkler': textdistance.jaro_winkler,
        'levenshtein': lambda x, y: 1 - (textdistance.levenshtein(x, y) / max(len(x), len(y))),
        'jaccard': textdistance.jaccard,
        'sorensen': textdistance.sorensen
    }
    
    scores = []
    for algo_name, weight in weights.items():
        if algo_name in algorithms:
            score = algorithms[algo_name](s1, s2)
            scores.append(score * weight)
    
    return sum(scores) / sum(weights.values())

# 使用示例
s1 = "Christopher"
s2 = "Kristopher"
print("混合相似度:", hybrid_similarity(s1, s2))
```
