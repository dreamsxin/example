# Go 语言性能优化

## 优化工作流

不断重复：建立评估指标(eg. Latency) → 定位瓶颈(一般都会定位到某个局部) → 寻找局部解决问题方案 → 尝试方案

## 问题定位工具

### pprof

#### 生成方式

- runtime/pprof: 手动调用如runtime.StartCPUProfile或者runtime.StopCPUProfile等 API 来生成和写入采样文件，灵活性高。主要用于本地测试。
- net/http/pprof: 通过 http 服务获取 Profile 采样文件，简单易用，适用于对应用程序的整体监控。通过 runtime/pprof 实现。主要用于服务器端测试。
- go test: 通过 go test -bench . -cpuprofile cpuprofile.out 生成采样文件，主要用于本地基准测试。可用于重点测试某些函数。

#### 查看方式

`go tool pprof [options][binary] ...`
–text 纯文本
–web 生成 svg 并用浏览器打开（如果 svg 的默认打开方式是浏览器)
–svg 只生成 svg
–list funcname 筛选出正则匹配 funcname 的函数的信息
-http=”:port” 直接本地浏览器打开 profile 查看（包括 top，graph，火焰图等）
go tool pprof -base profile1 profile2
对比查看 2 个 profile，一般用于代码修改前后对比，定位差异点。

通过命令行方式查看 profile 时，可以在命令行对话中，使用下列命令，查看相关信息

flat flat%   sum%        cum   cum%
5.95s 27.56% 27.56%      5.95s 27.56%  runtime.usleep
4.97s 23.02% 50.58%      5.08s 23.53%  sync.(*RWMutex).RLock
4.46s 20.66% 71.24%      4.46s 20.66%  sync.(*RWMutex).RUnlock
2.69s 12.46% 83.70%      2.69s 12.46%  runtime.pthread_cond_wait
1.50s  6.95% 90.64%      1.50s  6.95%  runtime.pthread_cond_signal
lat: 采样时，该函数正在运行的次数*采样频率(10ms)，即得到估算的函数运行”采样时间”。这里不包括函数等待子函数返回。
flat%: flat / 总采样时间值
sum%: 前面所有行的 flat% 的累加值，如第三行 sum% = 71.24% = 27.56% + 50.58%
cum: 采样时，该函数出现在调用堆栈的采样时间，包括函数等待子函数返回。因此 flat <= cum
cum%: cum / 总采样时间值
topN [-cum] 查看前 N 个数据：

list ncname 查看某个函数的详细信息，可以明确具体的资源（cpu，内存等）是由哪一行触发的。

基本原理：

每秒被唤醒 100 次，记录每个线程上的栈，那些等待 IO 被 gopark 之类挂起的 goroutine 不会被采集到，因为不在线程上运行，gopark 挂起 goroutine 后，当前线程一般会进 schedule → findrunnable 的调度循环。
#### pprof 接入

服务中 main 方法插入代码
```go
cfg := tars.GetServerConfig()
profMux := &tars.TarsHttpMux{}
profMux.HandleFunc("/debug/pprof/", pprof.Index)
profMux.HandleFunc("/debug/pprof/cmdline", pprof.Cmdline)
profMux.HandleFunc("/debug/pprof/profile", pprof.Profile)
profMux.HandleFunc("/debug/pprof/symbol", pprof.Symbol)
profMux.HandleFunc("/debug/pprof/trace", pprof.Trace)
tars.AddHttpServant(profMux, cfg.App+"."+cfg.Server+".ProfObj")
```

#### 查看服务的 pprof

保证开发机能直接访问到节点部署的 ip 和 port。
查看 profile(http 地址中的 ip,port 为 ProfObj 的 ip 和 port)
```shell
# 下载cpu profile
go tool pprof http://ip:port/debug/pprof/profile?seconds=120 # 等待120s，不带此参数时等待30s

# 下载heap profile
go tool pprof http://ip:port/debug/pprof/heap

# 下载goroutine profile
go tool pprof http://ip:port/debug/pprof/goroutine

# 下载block profile
go tool pprof http://ip:port/debug/pprof/block

# 下载mutex profile
go tool pprof http://ip:port/debug/pprof/mutex

# 下载20秒的trace记录（遇到棘手问题时，查看trace会比较容易定位)
 curl http://100.97.1.35:10078/debug/pprof/trace?seconds=20 > trace.out
 go tool trace trace.out 查看
```
直接在终端中通过 pprof 命令查看

sz 上面命令执行时出现的Saved profile in /root/pprof/pprof.binary.alloc_objects.xxxxxxx.xxxx.pb.gz到本地
在本地环境，执行 `go tool pprof -http=":8081" pprof.binary.alloc_objects.xxxxxxx.xxxx.pb.gz` 即可直接通过http://localhost:8081页面查看。包括topN，火焰图信息等,会更方便一点。

### GC Trace

golang 具备 GC 功能，而 GC 是最容易被忽视的性能影响因素。尤其是在本地使用 benchmark 测试时，由于时间较短，占用内存较少。往往不会触发 GC。而一旦线上出现 GC 问题，又不太好定位。目前常用的定位方式有两种：

本地 gctrace
在执行程序前加 GODEBUG=gctrace=1，每次 gc 时会输出一行如下内容

gc 1 @0.001s 11%: 0.007+1.5+0.004 ms clock, 0.089+1.5/2.8/0.27+0.054 ms cpu, 4->4->3 MB, 5 MB goal, 12 P
scvg: inuse: 4, idle: 57, sys: 62, released: 57, consumed: 4 (MB)
也通过日志转为图形化：

GODEBUG=gctrace=1 godoc -index -http=:6060 2> stderr.log
cat stderr.log | gcvis
inuse：使用了多少 M 内存
idle：剩下要清除的内存
sys：系统映射的内存
released：释放的系统内存
consumed：申请的系统内存
gc 1 表示第 1 次 gc
@0.001s 表示程序执行的总时间
11% 表示垃圾回收时间占用总的运行时间百分比
0.007+1.5+0.004 ms clock 表示工作线程完成 GC 的 stop-the-world,sweeping,marking 和 waiting 的时间
0.089+1.5/2.8/0.27+0.054 ms cpu 垃圾回收占用 cpu 时间
4->4->3 MB 表示堆的大小，gc 后堆的大小，存活堆的大小
5 MB goal 整体堆的大小
12 P 使用的处理器数量
scvg: inuse: 4, idle: 57, sys: 62, released: 57, consumed: 4 (MB) 表示系统内存回收信息
采用图形化的方式查看：https://github.com/davecheney/gcvis
GODEBUG=gctrace=1 go test -v *.go -bench=. -run=none -benchtime 3m |& gcvis
线上 trace
在线上业务中添加net/http/pprof后，可通过下列命令采集 20 秒的 trace 信息

curl http://ip:port/debug/pprof/trace?seconds=20 > trace.out
再通过go tool trace trace.out 即可在本地浏览器中查看 trace 信息。

### fgprof

比较类似，但是会包含那些 Off CPU 的 goroutine。比如可以结合该库与 goroutine 的增长情况来做一段逻辑：当 goroutine 突然增长时，用 fgprof 采样 x 秒，可以发现是在代码的什么位置发生了阻塞。当然，也可以直接把 pprof 的 goroutine stack 给 dump 下来。

### trace
一般用来诊断一些诡异的抖动问题，或 runtime 的 bug(或者用来学习 runtime 的执行流)，用来做问题诊断效果一般。

基本原理是在 runtime 中埋了大量点，记录一堆 event 来追踪 runtime 执行流程。

如果对一些调度问题有疑问，可以在 trace 里做观察，不过拿来定位问题还是比较费劲的。

https://xargin.com/a-rlock-story/

### perf

perf 也是可以用的，比如线上没开 pprof 的时候，发现 CPU 炸了，perf 可以看看到底在干啥，因为 Go 默认会把 DWARF 调试信息带进二进制文件中，通过 perf 的 zoom 功能也可以一直看到哪行代码(或者是汇编)占用了比较高的 CPU。
```shell
$ perf stat -e task-clock,cycles,instructions,cache-references,cache-misses ./hello
yyyy

 Performance counter stats for './hello':

          1.464376      task-clock (msec)         #    0.979 CPUs utilized
         3,681,288      cycles                    #    2.514 GHz
         1,722,170      instructions              #    0.47  insn per cycle
            46,475      cache-references          #   31.737 M/sec
            21,479      cache-misses              #   46.216 % of all cache refs

       0.001495925 seconds time elapsed
```
- perf top

## 局部优化

### benchmark 简介
`go test -v gate_test.go -run=none -bench=. -benchtime=3s -cpuprofile cpu.prof -memprofile mem.prof`
-run 知道单次测试，一般用于代码逻辑验证
-bench=. 执行所有 Benchmark，也可以通过用例函数名来指定部分测试用例
-benchtime 指定测试执行时长
-cpuprofile 输出 cpu 的 pprof 信息文件
-memprofile 输出 heap 的 pprof 信息文件。
-blockprofile 阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置
-mutexprofile 互斥锁分析，报告互斥锁的竞争情况
benchmark 测试用例常用函数

b.ReportAllocs() 输出单次循环使用的内存数量和对象 allocs 信息
b.RunParallel() 使用协程并发测试
b.SetBytes(n int64) 设置单次循环使用的内存数量

`go test -bench=. -benchmem`

或者

`go test -cpuprofile -bench`

memprofile 同理，一次只 bench 一种，否则可能不准。

## 全局优化

寻找程序的整体瓶颈。

- wrk、pprof、压测平台

- https://github.com/bojand/ghz

有压测平台是最好的，方便 AB，自己玩比较容易手忙脚乱，数据错位(压测的时候收集数据写报告经常容易张冠李戴，导致返工，还是有平台安逸)。

## 性能瓶颈举例

### 调用外部命令

### 序列化 CPU 占用过高

### 算法时间复杂度

### 过多的系统调用(合并调用)

### sync.Pool

### offheap

### 减少指针类型变量逃逸

使用 go build -gcflags="-m -m" 来分析逃逸。

如果要分析某个 package 内的逃逸情况，可以打全 package 名，例如 go build -gcflags="-m -m" github.com/cch123/elasticsql

string 类型天然就是带指针的类型，比如一些 cache 服务，有几千万 entry，那么用 string 来做 key 和 value 可能成本就很高。

### map 结构的 128 阈值

###过多的调度 CPU 占用(例如火焰图中，schedule 有一大条)

### 锁冲突

map → sync.Map(读多写少)

换用无锁结构，如 lock free queue、stack 等

分段锁

copy on write，业务逻辑允许的前提下，在修改时拷贝一份，再修改

## 持续分析工具

https://github.com/grafana/pyroscope/tree/main/examples/golang-push

