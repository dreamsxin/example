# TimescaleDB 数据分层处理方案

下面我将详细介绍如何在 TimescaleDB 中实现数据分层处理，优化存储成本并保持查询性能。

## 数据分层架构设计

### 1. 分层策略设计

```
数据生命周期：
热数据 (0-30天) → 温数据 (30-90天) → 冷数据 (90-365天) → 归档数据 (365天以上)
```

### 2. 创建分层存储策略

```sql
-- 创建分层配置表
CREATE TABLE data_tiering_config (
    tier_name VARCHAR(50) PRIMARY KEY,
    retention_period INTERVAL NOT NULL,
    compression_enabled BOOLEAN DEFAULT true,
    tier_priority INTEGER NOT NULL,
    description TEXT
);

-- 配置分层策略
INSERT INTO data_tiering_config VALUES
('hot', '30 days', false, 1, '热数据：频繁访问，不压缩'),
('warm', '90 days', true, 2, '温数据：偶尔访问，启用压缩'),
('cold', '365 days', true, 3, '冷数据：很少访问，高度压缩'),
('archive', '5 years', true, 4, '归档数据：极少访问，最大压缩');
```

## 分层存储实现

### 1. 按时间范围创建分区

```sql
-- 修改用户事件表的分区策略
SELECT remove_compression_policy('user_events');
SELECT remove_retention_policy('user_events');

-- 重新配置分区策略
SELECT set_chunk_time_interval('user_events', INTERVAL '1 day');

-- 创建分区函数
CREATE OR REPLACE FUNCTION get_chunk_tier(chunk_id INTEGER)
RETURNS VARCHAR(50) AS $$
DECLARE
    chunk_range TSTZRANGE;
    chunk_age INTERVAL;
BEGIN
    -- 获取分区的数据时间范围
    SELECT range INTO chunk_range
    FROM timescaledb_information.chunks 
    WHERE chunk_id = $1;
    
    -- 计算分区中最新数据的年龄
    chunk_age = NOW() - upper(chunk_range);
    
    -- 根据数据年龄确定分层
    IF chunk_age < INTERVAL '30 days' THEN
        RETURN 'hot';
    ELSIF chunk_age < INTERVAL '90 days' THEN
        RETURN 'warm';
    ELSIF chunk_age < INTERVAL '365 days' THEN
        RETURN 'cold';
    ELSE
        RETURN 'archive';
    END IF;
END;
$$ LANGUAGE plpgsql;
```

### 2. 实施分层存储策略

```sql
-- 创建分层管理函数
CREATE OR REPLACE FUNCTION apply_tiering_policy()
RETURNS VOID AS $$
DECLARE
    chunk_record RECORD;
    tier_name VARCHAR(50);
BEGIN
    -- 遍历所有分区
    FOR chunk_record IN 
        SELECT chunk_id, chunk_name, range
        FROM timescaledb_information.chunks 
        WHERE hypertable_name = 'user_events'
    LOOP
        -- 确定分区所属层级
        tier_name := get_chunk_tier(chunk_record.chunk_id);
        
        -- 根据层级应用不同的策略
        CASE tier_name
            WHEN 'hot' THEN
                -- 热数据：禁用压缩，确保最佳查询性能
                PERFORM alter_chunk_compression(chunk_record.chunk_name, false);
                
            WHEN 'warm' THEN
                -- 温数据：启用压缩，平衡性能与存储
                PERFORM alter_chunk_compression(chunk_record.chunk_name, true);
                PERFORM set_chunk_compression_settings(
                    chunk_record.chunk_name,
                    compress_orderby => 'event_time DESC',
                    compress_segmentby => 'user_id, event_name_id'
                );
                
            WHEN 'cold' THEN
                -- 冷数据：启用高级压缩
                PERFORM alter_chunk_compression(chunk_record.chunk_name, true);
                PERFORM set_chunk_compression_settings(
                    chunk_record.chunk_name,
                    compress_orderby => 'event_time DESC',
                    compress_segmentby => 'user_id',
                    compress_ratio => 0.8
                );
                
            WHEN 'archive' THEN
                -- 归档数据：最大压缩
                PERFORM alter_chunk_compression(chunk_record.chunk_name, true);
                PERFORM set_chunk_compression_settings(
                    chunk_record.chunk_name,
                    compress_orderby => 'event_time DESC',
                    compress_segmentby => 'user_id',
                    compress_ratio => 0.9
                );
        END CASE;
    END LOOP;
END;
$$ LANGUAGE plpgsql;

-- 创建自动执行分层策略的作业
SELECT add_job('apply_tiering_policy', INTERVAL '1 hour');
```

## 分层查询优化

### 1. 创建分层查询视图

```sql
-- 创建热数据视图（最近30天）
CREATE VIEW hot_user_events AS
SELECT * FROM user_events
WHERE event_time >= NOW() - INTERVAL '30 days';

-- 创建温数据视图（30-90天）
CREATE VIEW warm_user_events AS
SELECT * FROM user_events
WHERE event_time >= NOW() - INTERVAL '90 days'
AND event_time < NOW() - INTERVAL '30 days';

-- 创建冷数据视图（90-365天）
CREATE VIEW cold_user_events AS
SELECT * FROM user_events
WHERE event_time >= NOW() - INTERVAL '365 days'
AND event_time < NOW() - INTERVAL '90 days';

-- 创建归档数据视图（365天以上）
CREATE VIEW archive_user_events AS
SELECT * FROM user_events
WHERE event_time < NOW() - INTERVAL '365 days';
```

### 2. 分层聚合表

```sql
-- 创建热数据聚合表（详细数据）
CREATE TABLE hot_aggregates (
    bucket TIMESTAMPTZ NOT NULL,
    event_name_id INTEGER,
    user_count INTEGER,
    event_count BIGINT,
    PRIMARY KEY (bucket, event_name_id)
);

SELECT create_hypertable('hot_aggregates', 'bucket');

-- 创建温数据聚合表（较粗粒度）
CREATE TABLE warm_aggregates (
    bucket TIMESTAMPTZ NOT NULL,
    event_name_id INTEGER,
    user_count INTEGER,
    event_count BIGINT,
    PRIMARY KEY (bucket, event_name_id)
);

SELECT create_hypertable('warm_aggregates', 'bucket', 
    chunk_time_interval => INTERVAL '7 days');

-- 创建冷数据聚合表（更粗粒度）
CREATE TABLE cold_aggregates (
    bucket TIMESTAMPTZ NOT NULL,
    event_name_id INTEGER,
    user_count INTEGER,
    event_count BIGINT,
    PRIMARY KEY (bucket, event_name_id)
);

SELECT create_hypertable('cold_aggregates', 'bucket', 
    chunk_time_interval => INTERVAL '30 days');
```

## 数据迁移和聚合策略

### 1. 创建数据降粒度聚合函数

```sql
-- 热数据到温数据的聚合迁移
CREATE OR REPLACE FUNCTION migrate_hot_to_warm()
RETURNS VOID AS $$
BEGIN
    -- 将30-90天的数据聚合到温数据表
    INSERT INTO warm_aggregates (bucket, event_name_id, user_count, event_count)
    SELECT 
        time_bucket('1 hour', event_time) as bucket,
        event_name_id,
        COUNT(DISTINCT user_id) as user_count,
        COUNT(*) as event_count
    FROM user_events
    WHERE event_time >= NOW() - INTERVAL '90 days'
    AND event_time < NOW() - INTERVAL '30 days'
    GROUP BY time_bucket('1 hour', event_time), event_name_id
    ON CONFLICT (bucket, event_name_id) DO UPDATE
    SET 
        user_count = EXCLUDED.user_count,
        event_count = EXCLUDED.event_count;
END;
$$ LANGUAGE plpgsql;

-- 温数据到冷数据的聚合迁移
CREATE OR REPLACE FUNCTION migrate_warm_to_cold()
RETURNS VOID AS $$
BEGIN
    -- 将90-365天的数据聚合到冷数据表
    INSERT INTO cold_aggregates (bucket, event_name_id, user_count, event_count)
    SELECT 
        time_bucket('1 day', event_time) as bucket,
        event_name_id,
        COUNT(DISTINCT user_id) as user_count,
        COUNT(*) as event_count
    FROM user_events
    WHERE event_time >= NOW() - INTERVAL '365 days'
    AND event_time < NOW() - INTERVAL '90 days'
    GROUP BY time_bucket('1 day', event_time), event_name_id
    ON CONFLICT (bucket, event_name_id) DO UPDATE
    SET 
        user_count = EXCLUDED.user_count,
        event_count = EXCLUDED.event_count;
END;
$$ LANGUAGE plpgsql;
```

### 2. 创建数据生命周期管理作业

```sql
-- 添加数据迁移作业
SELECT add_job('migrate_hot_to_warm', INTERVAL '1 day');
SELECT add_job('migrate_warm_to_cold', INTERVAL '7 days');

-- 添加数据清理作业（归档超过5年的数据）
SELECT add_retention_policy('user_events', INTERVAL '5 years');

-- 添加聚合数据清理作业
SELECT add_retention_policy('hot_aggregates', INTERVAL '90 days');
SELECT add_retention_policy('warm_aggregates', INTERVAL '365 days');
SELECT add_retention_policy('cold_aggregates', INTERVAL '5 years');
```

## 分层查询接口

### 1. 创建统一查询函数

```sql
-- 创建智能查询函数，自动选择合适的数据层
CREATE OR REPLACE FUNCTION query_user_events(
    p_start_time TIMESTAMPTZ,
    p_end_time TIMESTAMPTZ,
    p_granularity INTERVAL DEFAULT INTERVAL '1 hour'
)
RETURNS TABLE (
    bucket TIMESTAMPTZ,
    event_name VARCHAR,
    user_count BIGINT,
    event_count BIGINT
) AS $$
DECLARE
    data_age INTERVAL;
BEGIN
    data_age := NOW() - p_start_time;
    
    -- 根据查询时间范围自动选择数据源
    IF data_age < INTERVAL '30 days' THEN
        -- 查询热数据
        RETURN QUERY
        SELECT 
            time_bucket(p_granularity, ue.event_time) as bucket,
            en.event_name,
            COUNT(DISTINCT ue.user_id) as user_count,
            COUNT(*) as event_count
        FROM user_events ue
        JOIN dim_event_names en ON ue.event_name_id = en.event_name_id
        WHERE ue.event_time BETWEEN p_start_time AND p_end_time
        GROUP BY time_bucket(p_granularity, ue.event_time), en.event_name;
        
    ELSIF data_age < INTERVAL '90 days' THEN
        -- 查询热数据聚合
        RETURN QUERY
        SELECT 
            time_bucket(p_granularity, ha.bucket) as bucket,
            en.event_name,
            SUM(ha.user_count) as user_count,
            SUM(ha.event_count) as event_count
        FROM hot_aggregates ha
        JOIN dim_event_names en ON ha.event_name_id = en.event_name_id
        WHERE ha.bucket BETWEEN p_start_time AND p_end_time
        GROUP BY time_bucket(p_granularity, ha.bucket), en.event_name;
        
    ELSIF data_age < INTERVAL '365 days' THEN
        -- 查询温数据聚合
        RETURN QUERY
        SELECT 
            time_bucket(p_granularity, wa.bucket) as bucket,
            en.event_name,
            SUM(wa.user_count) as user_count,
            SUM(wa.event_count) as event_count
        FROM warm_aggregates wa
        JOIN dim_event_names en ON wa.event_name_id = en.event_name_id
        WHERE wa.bucket BETWEEN p_start_time AND p_end_time
        GROUP BY time_bucket(p_granularity, wa.bucket), en.event_name;
        
    ELSE
        -- 查询冷数据聚合
        RETURN QUERY
        SELECT 
            time_bucket(p_granularity, ca.bucket) as bucket,
            en.event_name,
            SUM(ca.user_count) as user_count,
            SUM(ca.event_count) as event_count
        FROM cold_aggregates ca
        JOIN dim_event_names en ON ca.event_name_id = en.event_name_id
        WHERE ca.bucket BETWEEN p_start_time AND p_end_time
        GROUP BY time_bucket(p_granularity, ca.bucket), en.event_name;
        
    END IF;
END;
$$ LANGUAGE plpgsql;
```

### 2. 创建分层统计函数

```sql
-- 分层统计函数
CREATE OR REPLACE FUNCTION get_tier_statistics()
RETURNS TABLE (
    tier_name VARCHAR,
    chunk_count BIGINT,
    total_size TEXT,
    compressed_size TEXT,
    compression_ratio NUMERIC,
    oldest_data TIMESTAMPTZ,
    newest_data TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    WITH chunk_stats AS (
        SELECT 
            get_chunk_tier(c.chunk_id) as tier,
            c.chunk_name,
            pg_total_relation_size(c.chunk_name) as total_size,
            pg_total_relation_size(c.chunk_name) FILTER (
                WHERE pcm.chunk_id IS NOT NULL
            ) as compressed_size,
            MIN(lower(c.range)) as min_time,
            MAX(upper(c.range)) as max_time
        FROM timescaledb_information.chunks c
        LEFT JOIN timescaledb_information.compressed_chunk_stats pcm 
            ON c.chunk_name = pcm.chunk_name
        WHERE c.hypertable_name = 'user_events'
        GROUP BY c.chunk_id, c.chunk_name
    )
    SELECT 
        cs.tier as tier_name,
        COUNT(*) as chunk_count,
        pg_size_pretty(SUM(cs.total_size)) as total_size,
        pg_size_pretty(SUM(cs.compressed_size)) as compressed_size,
        CASE 
            WHEN SUM(cs.total_size) > 0 THEN 
                100.0 * SUM(cs.compressed_size) / SUM(cs.total_size)
            ELSE 0 
        END as compression_ratio,
        MIN(cs.min_time) as oldest_data,
        MAX(cs.max_time) as newest_data
    FROM chunk_stats cs
    GROUP BY cs.tier
    ORDER BY 
        CASE cs.tier
            WHEN 'hot' THEN 1
            WHEN 'warm' THEN 2
            WHEN 'cold' THEN 3
            WHEN 'archive' THEN 4
        END;
END;
$$ LANGUAGE plpgsql;
```

## Go 代码集成

### 1. 更新数据访问层

```go
// 添加分层查询方法
func (r *EventRepository) QueryTieredEvents(start, end time.Time, granularity string) ([]TieredEventResult, error) {
    var results []TieredEventResult
    
    query := `
        SELECT * FROM query_user_events($1, $2, $3::interval)
    `
    
    rows, err := r.pool.Query(context.Background(), query, start, end, granularity)
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    for rows.Next() {
        var result TieredEventResult
        if err := rows.Scan(
            &result.Bucket,
            &result.EventName,
            &result.UserCount,
            &result.EventCount,
        ); err != nil {
            return nil, err
        }
        results = append(results, result)
    }
    
    return results, nil
}

// 添加分层统计方法
func (r *EventRepository) GetTierStatistics() ([]TierStatistic, error) {
    var stats []TierStatistic
    
    rows, err := r.pool.Query(context.Background(), "SELECT * FROM get_tier_statistics()")
    if err != nil {
        return nil, err
    }
    defer rows.Close()
    
    for rows.Next() {
        var stat TierStatistic
        if err := rows.Scan(
            &stat.TierName,
            &stat.ChunkCount,
            &stat.TotalSize,
            &stat.CompressedSize,
            &stat.CompressionRatio,
            &stat.OldestData,
            &stat.NewestData,
        ); err != nil {
            return nil, err
        }
        stats = append(stats, stat)
    }
    
    return stats, nil
}
```

### 2. 添加数据模型

```go
type TieredEventResult struct {
    Bucket     time.Time `json:"bucket"`
    EventName  string    `json:"event_name"`
    UserCount  int64     `json:"user_count"`
    EventCount int64     `json:"event_count"`
}

type TierStatistic struct {
    TierName         string    `json:"tier_name"`
    ChunkCount       int64     `json:"chunk_count"`
    TotalSize        string    `json:"total_size"`
    CompressedSize   string    `json:"compressed_size"`
    CompressionRatio float64   `json:"compression_ratio"`
    OldestData       time.Time `json:"oldest_data"`
    NewestData       time.Time `json:"newest_data"`
}
```

## 监控和告警

### 1. 创建分层监控

```sql
-- 创建分层监控表
CREATE TABLE tier_monitoring (
    monitor_time TIMESTAMPTZ DEFAULT NOW(),
    tier_name VARCHAR(50),
    chunk_count INTEGER,
    total_size BIGINT,
    compressed_size BIGINT,
    compression_ratio NUMERIC,
    PRIMARY KEY (monitor_time, tier_name)
);

SELECT create_hypertable('tier_monitoring', 'monitor_time');

-- 创建监控数据收集函数
CREATE OR REPLACE FUNCTION collect_tier_monitoring()
RETURNS VOID AS $$
BEGIN
    INSERT INTO tier_monitoring (tier_name, chunk_count, total_size, compressed_size, compression_ratio)
    SELECT 
        tier_name,
        chunk_count,
        pg_total_relation_size::BIGINT,
        pg_compressed_relation_size::BIGINT,
        compression_ratio
    FROM get_tier_statistics();
END;
$$ LANGUAGE plpgsql;

-- 添加监控作业
SELECT add_job('collect_tier_monitoring', INTERVAL '1 hour');
```

### 2. 创建告警规则

```sql
-- 创建分层告警规则
CREATE TABLE tier_alerts (
    alert_id SERIAL PRIMARY KEY,
    alert_time TIMESTAMPTZ DEFAULT NOW(),
    tier_name VARCHAR(50),
    alert_type VARCHAR(50),
    alert_message TEXT,
    severity VARCHAR(20),
    resolved BOOLEAN DEFAULT false
);

-- 创建检查分层健康的函数
CREATE OR REPLACE FUNCTION check_tier_health()
RETURNS VOID AS $$
DECLARE
    tier_stats RECORD;
BEGIN
    FOR tier_stats IN SELECT * FROM get_tier_statistics() LOOP
        -- 检查压缩率异常
        IF tier_stats.compression_ratio > 90 THEN
            INSERT INTO tier_alerts (tier_name, alert_type, alert_message, severity)
            VALUES (
                tier_stats.tier_name,
                'HIGH_COMPRESSION',
                'Compression ratio exceeds 90%: ' || tier_stats.compression_ratio || '%',
                'WARNING'
            );
        END IF;
        
        -- 检查存储空间增长过快
        -- 这里可以添加更多的健康检查逻辑
    END LOOP;
END;
$$ LANGUAGE plpgsql;

-- 添加健康检查作业
SELECT add_job('check_tier_health', INTERVAL '6 hours');
```

## 总结

通过实现数据分层处理，我们获得了以下好处：

1. **成本优化**：根据数据热度采用不同的存储策略，降低存储成本
2. **性能优化**：热数据保持高性能，冷数据采用压缩节省空间
3. **自动化管理**：自动的数据迁移、压缩和清理策略
4. **统一查询接口**：对应用层透明，自动选择合适的数据源
5. **监控告警**：全面的监控和健康检查机制

这种分层架构特别适合用户行为分析系统，其中数据的热度随时间快速下降。新的数据被频繁查询，而旧的数据主要用于历史分析和报表生成。
