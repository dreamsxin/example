## ğŸ“Œ å¦‚ä½•ä½¿ç”¨Llamaæ¨¡å‹?


ä½ å¯ä»¥é€‰æ‹©ä¸‹é¢çš„å¿«é€Ÿä¸Šæ‰‹çš„ä»»ä¸€ç§æ–¹å¼ï¼Œå¼€å§‹ä½¿ç”¨ Llama ç³»åˆ—æ¨¡å‹ã€‚æ¨èä½¿ç”¨[ä¸­æ–‡é¢„è®­ç»ƒå¯¹è¯æ¨¡å‹](#llama2ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹atom-7b)è¿›è¡Œä½¿ç”¨ï¼Œå¯¹ä¸­æ–‡çš„æ•ˆæœæ”¯æŒæ›´å¥½ã€‚


### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨Anaconda

ç¬¬ 0 æ­¥ï¼šå‰ææ¡ä»¶
- ç¡®ä¿å®‰è£…äº† Python 3.10 ä»¥ä¸Šç‰ˆæœ¬ã€‚

ç¬¬ 1 æ­¥ï¼šå‡†å¤‡ç¯å¢ƒ

å¦‚éœ€è®¾ç½®ç¯å¢ƒï¼Œå®‰è£…æ‰€éœ€è¦çš„è½¯ä»¶åŒ…ï¼Œè¿è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚
```bash
git clone https://github.com/LlamaFamily/Llama-Chinese.git
cd Llama-Chinese
pip install -r requirements.txt
```

ç¬¬ 2 æ­¥ï¼šä¸‹è½½æ¨¡å‹

ä½ å¯ä»¥ä»ä»¥ä¸‹æ¥æºä¸‹è½½Atom-7B-Chatæ¨¡å‹ã€‚
- [HuggingFace](https://huggingface.co/FlagAlpha)
- [ModelScope](https://modelscope.cn/organization/FlagAlpha)
- [WideModel](https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat)

ç¬¬ 3 æ­¥ï¼šè¿›è¡Œæ¨ç†

ä½¿ç”¨Atom-7B-Chatæ¨¡å‹è¿›è¡Œæ¨ç†
åˆ›å»ºä¸€ä¸ªåä¸º quick_start.py çš„æ–‡ä»¶ï¼Œå¹¶å°†ä»¥ä¸‹å†…å®¹å¤åˆ¶åˆ°è¯¥æ–‡ä»¶ä¸­ã€‚
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
device_map = "cuda:0" if torch.cuda.is_available() else "auto"
model = AutoModelForCausalLM.from_pretrained('FlagAlpha/Atom-7B-Chat',device_map=device_map,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True,use_flash_attention_2=True)
model =model.eval()
tokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Atom-7B-Chat',use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
input_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹ä¸­å›½\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids
if torch.cuda.is_available():
  input_ids = input_ids.to('cuda')
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

è¿è¡Œ quick_start.py ä»£ç ã€‚
```bash
python quick_start.py
```

### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨Docker

è¯¦æƒ…å‚è§ï¼š[Dockeréƒ¨ç½²](https://github.com/LlamaFamily/Llama-Chinese/blob/main/docs/chat_gradio_guide.md)

ç¬¬ 1 æ­¥ï¼šå‡†å¤‡dockeré•œåƒï¼Œé€šè¿‡dockerå®¹å™¨å¯åŠ¨[chat_gradio.py](../examples/chat_gradio.py)
```bash
git clone https://github.com/LlamaFamily/Llama-Chinese.git

cd Llama-Chinese

docker build -f docker/Dockerfile -t flagalpha/llama2-chinese:gradio .
```

ç¬¬ 2 æ­¥ï¼šé€šè¿‡docker-composeå¯åŠ¨chat_gradio
```bash
cd Llama-Chinese/docker
docker-compose up -d --build
```

### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨llama.cpp
è¯¦æƒ…å‚è§ï¼š[ä½¿ç”¨llama.cpp](https://github.com/LlamaFamily/Llama-Chinese/blob/main/inference-speed/CPU/ggml/README.md)

### å¿«é€Ÿä¸Šæ‰‹-ä½¿ç”¨gradio
åŸºäºgradioæ­å»ºçš„é—®ç­”ç•Œé¢ï¼Œå®ç°äº†æµå¼çš„è¾“å‡ºï¼Œå°†ä¸‹é¢ä»£ç å¤åˆ¶åˆ°æ§åˆ¶å°è¿è¡Œï¼Œä»¥ä¸‹ä»£ç ä»¥Atom-7B-Chatæ¨¡å‹ä¸ºä¾‹ï¼Œä¸åŒæ¨¡å‹åªéœ€ä¿®æ”¹ä¸€ä¸‹é¢çš„model_name_or_pathå¯¹åº”çš„æ¨¡å‹åç§°å°±å¥½äº†ğŸ˜Š
```
python examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B-Chat
```

### å¿«é€Ÿä¸Šæ‰‹-æ„å»ºAPIæœåŠ¡
ä½¿ç”¨FastChatæ„å»ºå’ŒOpenAIä¸€è‡´çš„æ¨ç†æœåŠ¡æ¥å£ã€‚

<details>
ç¬¬ 0 æ­¥ï¼šå‰ææ¡ä»¶

å®‰è£…fastchat
```bash
pip3 install "fschat[model_worker,webui]"
```
ç¬¬ 1 æ­¥ï¼šå¯åŠ¨Restful API

å¼€å¯ä¸‰ä¸ªæ§åˆ¶å°åˆ†åˆ«æ‰§è¡Œä¸‹é¢çš„ä¸‰ä¸ªå‘½ä»¤
- é¦–å…ˆå¯åŠ¨controler
```bash
python3 -m fastchat.serve.controller \
--host localhost \
--port 21001
```

- å¯åŠ¨æ¨¡å‹
```bash
CUDA_VISIBLE_DEVICES="0" python3 -m fastchat.serve.model_worker --model-path /path/Atom-7B-Chat \
--host localhost \
--port 21002 \
--worker-address "http://localhost:21002" \
--limit-worker-concurrency 5 \
--stream-interval 2 \
--gpus "1" \
--load-8bit
```

- å¯åŠ¨RESTful API æœåŠ¡
```bash
python3 -m fastchat.serve.openai_api_server \
--host localhost \
--port 21003 \
--controller-address http://localhost:21001
```

ç¬¬ 2 æ­¥ï¼šæµ‹è¯•apiæœåŠ¡

æ‰§è¡Œä¸‹é¢çš„pythonä»£ç æµ‹è¯•ä¸Šé¢éƒ¨ç½²çš„apiæœåŠ¡
```python
# coding=utf-8
import json
import time
import urllib.request
import sys
import requests

def test_api_server(input_text):
    header = {'Content-Type': 'application/json'}

    data = {
          "messages": [{"role": "system", "content": ""}, {"role": "user", "content": input_text}],
          "temperature": 0.3, 
          "top_p" : 0.95, 
          "max_tokens": 512, 
          "model": "LLama2-Chinese-13B",
          "stream" : False,
          "n" : 1,
          "best_of": 1, 
          "presence_penalty": 1.2, 
          "frequency_penalty": 0.2,           
          "top_k": 50, 
          "use_beam_search": False, 
          "stop": [], 
          "ignore_eos" :False,
          "logprobs": None
    }
    response = requests.post(
        url='http://127.0.0.1:21003/v1/chat/completions',
        headers=header,
        data=json.dumps(data).encode('utf-8')
    )

    result = None
    try:
        result = json.loads(response.content)
        print(json.dumps(data, ensure_ascii=False, indent=2))
        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(e)

    return result

if __name__ == "__main__":
    test_api_server("å¦‚ä½•å»åŒ—äº¬?")
```

</details>
