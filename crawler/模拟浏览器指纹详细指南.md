# mitmproxy 结合 curl_cffi 模拟浏览器指纹详细指南

将 mitmproxy 与 curl_cffi 结合，实现浏览器指纹的完美模拟。

## 1. 环境准备

### 安装必要的包
```bash
# 安装 mitmproxy
pip install mitmproxy

# 安装 curl_cffi（用于模拟浏览器 TLS 指纹）
pip install curl-cffi

# 安装 requests（可选，用于测试）
pip install requests
```

### 安装 mitmproxy 证书
```bash
# 启动 mitmproxy 一次以生成证书
mitmdump --mode regular --listen-port 8080

# 然后在另一个终端安装证书
# 找到证书位置（通常在 ~/.mitmproxy/mitmproxy-ca-cert.pem）
# 或者通过浏览器访问 http://mitm.it/ 下载证书
```

## 2. 创建 mitmproxy 插件脚本

创建一个名为 `browser_fingerprint.py` 的文件：

```python
# browser_fingerprint.py
from mitmproxy import http, ctx
import random
import time
import json

class BrowserFingerprintSimulator:
    def __init__(self):
        self.client_profiles = {}
        self.user_agents = [
            # Chrome on Windows
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
            # Chrome on macOS
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            # Firefox
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/119.0",
        ]
        
    def get_client_profile(self, client_id):
        """获取或创建客户端配置档案"""
        if client_id not in self.client_profiles:
            user_agent = random.choice(self.user_agents)
            self.client_profiles[client_id] = {
                'user_agent': user_agent,
                'browser_type': self.detect_browser_type(user_agent),
                'platform': self.detect_platform(user_agent),
                'created_at': time.time()
            }
        return self.client_profiles[client_id]
    
    def detect_browser_type(self, user_agent):
        """检测浏览器类型"""
        if "Chrome" in user_agent:
            return "chrome"
        elif "Firefox" in user_agent:
            return "firefox"
        elif "Safari" in user_agent:
            return "safari"
        else:
            return "chrome"
    
    def detect_platform(self, user_agent):
        """检测平台"""
        if "Windows" in user_agent:
            return "windows"
        elif "Mac" in user_agent:
            return "macos"
        elif "Linux" in user_agent:
            return "linux"
        else:
            return "windows"
    
    def get_sec_headers(self, browser_type, platform):
        """生成安全相关头部"""
        if browser_type == "chrome":
            return {
                "sec-ch-ua": '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": f'"{platform.capitalize()}"',
            }
        elif browser_type == "firefox":
            return {
                "sec-ch-ua": '"Not.A/Brand";v="8", "Chromium";v="119", "Google Chrome";v="119"',
                "sec-ch-ua-mobile": "?0", 
                "sec-ch-ua-platform": f'"{platform.capitalize()}"',
            }
        else:
            return {
                "sec-ch-ua": '"Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": f'"{platform.capitalize()}"',
            }
    
    def get_accept_headers(self, browser_type):
        """生成 Accept 相关头部"""
        if browser_type == "firefox":
            return {
                "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "accept-language": "zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3",
                "accept-encoding": "gzip, deflate, br",
            }
        else:  # chrome and others
            return {
                "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
                "accept-encoding": "gzip, deflate, br, zstd",
            }
    
    def get_fetch_headers(self, request_method, browser_type):
        """生成 Fetch 相关头部"""
        if request_method.upper() == "GET":
            return {
                "sec-fetch-dest": "document",
                "sec-fetch-mode": "navigate",
                "sec-fetch-site": "none",
                "sec-fetch-user": "?1",
            }
        else:  # POST and others
            return {
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors", 
                "sec-fetch-site": "same-origin",
            }
    
    def request(self, flow: http.HTTPFlow):
        """修改请求以模拟浏览器指纹"""
        # 获取客户端ID（使用源IP作为标识）
        client_id = flow.client_conn.address[0] if flow.client_conn else "default"
        
        # 获取客户端配置
        profile = self.get_client_profile(client_id)
        
        # 移除可疑的代理头
        suspicious_headers = [
            'x-forwarded-for', 'x-forwarded-proto', 'via', 
            'x-real-ip', 'forwarded'
        ]
        for header in suspicious_headers:
            flow.request.headers.pop(header, None)
        
        # 设置浏览器指纹头
        headers = flow.request.headers
        
        # 1. 设置 User-Agent
        headers["user-agent"] = profile['user_agent']
        
        # 2. 设置安全头
        sec_headers = self.get_sec_headers(profile['browser_type'], profile['platform'])
        for key, value in sec_headers.items():
            headers[key] = value
        
        # 3. 设置 Accept 头
        accept_headers = self.get_accept_headers(profile['browser_type'])
        for key, value in accept_headers.items():
            headers[key] = value
        
        # 4. 设置 Fetch 头
        fetch_headers = self.get_fetch_headers(flow.request.method, profile['browser_type'])
        for key, value in fetch_headers.items():
            headers[key] = value
        
        # 5. 设置其他通用头
        headers["upgrade-insecure-requests"] = "1"
        headers["dnt"] = "1"
        headers["cache-control"] = "max-age=0"
        
        # 记录修改的请求（调试用）
        ctx.log.info(f"Modified request for {client_id}: {profile['browser_type']} on {profile['platform']}")
    
    def response(self, flow: http.HTTPFlow):
        """处理响应，可以用于后续请求的 Referer 设置"""
        # 这里可以存储一些状态信息，比如为下一个请求设置 Referer
        pass

addons = [BrowserFingerprintSimulator()]
```

## 3. 创建 curl_cffi 客户端代码

创建一个名为 `curl_cffi_client.py` 的文件：

```python
# curl_cffi_client.py
from curl_cffi import requests
import random
import time
import threading
from urllib.parse import urlparse
import json

class AdvancedBrowserClient:
    def __init__(self, proxy_url="http://127.0.0.1:8080"):
        self.proxy_url = proxy_url
        self.session_storage = {}
        self.lock = threading.Lock()
        
        # 浏览器模拟配置
        self.browser_profiles = [
            {"impersonate": "chrome110", "name": "Chrome 110"},
            {"impersonate": "chrome107", "name": "Chrome 107"},
            {"impersonate": "chrome104", "name": "Chrome 104"},
            {"impersonate": "chrome101", "name": "Chrome 101"},
            {"impersonate": "chrome100", "name": "Chrome 100"},
            {"impersonate": "chrome99", "name": "Chrome 99"},
            {"impersonate": "safari15_5", "name": "Safari 15.5"},
            {"impersonate": "safari15_3", "name": "Safari 15.3"},
        ]
    
    def get_session(self, session_id=None):
        """获取或创建会话"""
        if session_id is None:
            session_id = threading.current_thread().ident
            
        with self.lock:
            if session_id not in self.session_storage:
                # 为每个会话随机选择浏览器配置
                browser_profile = random.choice(self.browser_profiles)
                self.session_storage[session_id] = {
                    'session': requests.Session(),
                    'browser_profile': browser_profile,
                    'last_request_time': 0,
                    'request_count': 0
                }
            return self.session_storage[session_id]
    
    def make_request(self, url, method="GET", headers=None, data=None, delay=True, session_id=None):
        """
        使用 curl_cffi 和 mitmproxy 发送请求
        
        参数:
            url: 目标URL
            method: HTTP方法
            headers: 自定义头部
            data: 请求数据
            delay: 是否添加随机延迟
            session_id: 会话ID
        """
        session_data = self.get_session(session_id)
        session = session_data['session']
        browser_profile = session_data['browser_profile']
        
        # 添加人类化延迟
        if delay and session_data['request_count'] > 0:
            current_time = time.time()
            time_since_last = current_time - session_data['last_request_time']
            if time_since_last < random.uniform(1, 3):
                sleep_time = random.uniform(0.5, 2)
                time.sleep(sleep_time)
        
        # 准备请求参数
        request_kwargs = {
            'method': method,
            'url': url,
            'impersonate': browser_profile['impersonate'],
            'proxies': {
                'http': self.proxy_url,
                'https': self.proxy_url
            },
            'timeout': 30,
            'verify': False  # 注意：因为使用mitmproxy，需要关闭SSL验证
        }
        
        if headers:
            request_kwargs['headers'] = headers
        
        if data:
            if method.upper() in ['POST', 'PUT', 'PATCH']:
                if isinstance(data, (dict, list)):
                    request_kwargs['json'] = data
                else:
                    request_kwargs['data'] = data
        
        try:
            # 发送请求
            response = session.request(**request_kwargs)
            
            # 更新会话状态
            with self.lock:
                session_data['last_request_time'] = time.time()
                session_data['request_count'] += 1
            
            # 记录请求信息
            self.log_request(url, browser_profile['name'], response.status_code)
            
            return response
            
        except Exception as e:
            print(f"请求失败: {e}")
            return None
    
    def log_request(self, url, browser_profile, status_code):
        """记录请求信息"""
        domain = urlparse(url).netloc
        print(f"[{browser_profile}] {domain} -> 状态码: {status_code}")
    
    def get_fingerprint_test(self, test_url="https://httpbin.org/headers"):
        """测试当前指纹效果"""
        response = self.make_request(test_url, delay=False)
        if response and response.status_code == 200:
            headers = response.json()
            print("=== 指纹测试结果 ===")
            print(f"使用的浏览器模拟: {self.get_session()['browser_profile']['name']}")
            print("请求头信息:")
            for key, value in headers['headers'].items():
                if any(x in key.lower() for x in ['user-agent', 'accept', 'sec-', 'encoding']):
                    print(f"  {key}: {value}")
            return True
        else:
            print("指纹测试失败")
            return False

# 使用示例
if __name__ == "__main__":
    # 创建客户端实例
    client = AdvancedBrowserClient()
    
    # 测试指纹
    print("正在进行指纹测试...")
    client.get_fingerprint_test()
    
    # 示例：访问多个网站
    test_urls = [
        "https://httpbin.org/ip",
        "https://httpbin.org/user-agent", 
        "https://httpbin.org/headers"
    ]
    
    print("\n开始模拟访问...")
    for url in test_urls:
        response = client.make_request(url)
        if response:
            print(f"成功访问: {url}")
        time.sleep(1)
```

## 4. 高级爬虫示例

创建一个实际的爬虫示例 `advanced_crawler.py`：

```python
# advanced_crawler.py
from curl_cffi_client import AdvancedBrowserClient
import time
import random
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

class StealthyCrawler:
    def __init__(self, max_workers=3, proxy_url="http://127.0.0.1:8080"):
        self.client = AdvancedBrowserClient(proxy_url)
        self.max_workers = max_workers
        self.results = []
        
    def crawl_page(self, url, session_id=None):
        """爬取单个页面"""
        try:
            # 随机延迟
            time.sleep(random.uniform(1, 3))
            
            response = self.client.make_request(
                url, 
                session_id=session_id,
                headers={
                    'referer': 'https://www.google.com/',  # 模拟从Google跳转
                }
            )
            
            if response and response.status_code == 200:
                return {
                    'url': url,
                    'status': 'success',
                    'content_length': len(response.text),
                    'session_id': session_id
                }
            else:
                return {
                    'url': url,
                    'status': f'failed - {response.status_code if response else "no response"}',
                    'session_id': session_id
                }
                
        except Exception as e:
            return {
                'url': url,
                'status': f'error - {str(e)}',
                'session_id': session_id
            }
    
    def parallel_crawl(self, urls):
        """并行爬取多个URL"""
        print(f"开始并行爬取 {len(urls)} 个URL，使用 {self.max_workers} 个worker...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # 为每个worker分配一个session_id
            future_to_url = {
                executor.submit(self.crawl_page, url, i % self.max_workers): url 
                for i, url in enumerate(urls)
            }
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    result = future.result()
                    self.results.append(result)
                    print(f"完成: {url} -> {result['status']}")
                except Exception as e:
                    print(f"URL {url} 生成异常: {e}")
        
        # 输出统计信息
        success_count = sum(1 for r in self.results if r['status'] == 'success')
        print(f"\n爬取完成! 成功: {success_count}/{len(urls)}")
        
        return self.results

# 使用示例
if __name__ == "__main__":
    # 要爬取的URL列表
    test_urls = [
        "https://httpbin.org/ip",
        "https://httpbin.org/user-agent",
        "https://httpbin.org/headers",
        "https://httpbin.org/get?param=test1",
        "https://httpbin.org/get?param=test2",
        "https://httpbin.org/get?param=test3",
    ] * 2  # 重复列表以测试会话保持
    
    # 创建爬虫实例
    crawler = StealthyCrawler(max_workers=3)
    
    # 执行爬取
    results = crawler.parallel_crawl(test_urls)
    
    # 保存结果
    with open('crawl_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("结果已保存到 crawl_results.json")
```

## 5. 启动和运行

### 步骤1：启动 mitmproxy
```bash
# 终端1：启动 mitmproxy 并使用我们的插件
mitmdump -s browser_fingerprint.py --listen-port 8080 --set flow_detail=0
```

### 步骤2：运行指纹测试
```bash
# 终端2：测试指纹效果
python curl_cffi_client.py
```

### 步骤3：运行爬虫示例
```bash
# 终端3：运行高级爬虫
python advanced_crawler.py
```

## 6. 验证和调试

### 创建验证脚本 `verify_fingerprint.py`：
```python
# verify_fingerprint.py
import requests
from curl_cffi import requests as cffi_requests

def test_vanilla_requests():
    """测试普通 requests 库的指纹"""
    print("=== 普通 Requests 指纹 ===")
    try:
        response = requests.get("https://tls.browserleaks.com/json", timeout=10)
        print(f"状态码: {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"TLS JA3: {data.get('ja3')}")
            print(f"HTTP2指纹: {data.get('akamai')}")
    except Exception as e:
        print(f"失败: {e}")

def test_curl_cffi_with_proxy():
    """测试 curl_cffi 通过 mitmproxy 的指纹"""
    print("\n=== curl_cffi + mitmproxy 指纹 ===")
    try:
        response = cffi_requests.get(
            "https://tls.browserleaks.com/json",
            impersonate="chrome110",
            proxies={
                'http': 'http://127.0.0.1:8080',
                'https': 'http://127.0.0.1:8080'
            },
            verify=False,
            timeout=10
        )
        print(f"状态码: {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            print(f"TLS JA3: {data.get('ja3')}")
            print(f"HTTP2指纹: {data.get('akamai')}")
            print("✓ 指纹应该与真实Chrome浏览器相似")
    except Exception as e:
        print(f"失败: {e}")

if __name__ == "__main__":
    test_vanilla_requests()
    test_curl_cffi_with_proxy()
```

## 关键优势

这种组合方案提供了：

1. **TLS指纹模拟**：curl_cffi 模拟真实浏览器的 TLS 握手指纹
2. **HTTP头部模拟**：mitmproxy 修改 HTTP 头部使其像浏览器
3. **会话管理**：保持一致的会话行为
4. **人类化行为**：随机延迟和合理的请求模式
5. **并发控制**：合理的并发数量避免触发风控

## 注意事项

1. **证书信任**：确保在系统中安装了 mitmproxy 的 CA 证书
2. **性能平衡**：在速度和隐匿性之间找到平衡点
3. **目标适应性**：根据目标网站调整浏览器配置和请求频率
4. **错误处理**：实现完善的错误处理和重试机制

这个完整方案应该能够有效绕过大多数基于指纹识别的反爬系统。
