## CART

CART 算法所用的决策树为二叉树（binary tree），既每次总是将“母节点”（parent node）一分为二，分裂（split）为两个“子节点”（child node），直至到达终结点。
本质上，二叉树将“特征空间”（feature space）进行递归分割（recursive partitioning），每次总是沿着某个特征变量xj轴平行的方向进行切割，切成“矩形”或“超矩形”区域，既所谓“箱体”（boxes）。因此，分类树是一种通过分割特征空间进行分类的分类器（classifier as partition）。

决策树模型将特征空间分割为若干（超）矩形的终结点。在进行预测时，此预测值为该终结点所有训练样本的最常见类别（most commonly occurring class）。对于回归问题，此预测值为该终结点所有训练样本的平均值。
因此，在数学上，决策树为分段常值函数（piecewise constant function）这意味着决策树估计的函数不是连续函数。但这并不妨碍决策树称成为一种灵活而有用的算法，特别是作为“基学习器”（base learner）广泛用于随机森林与提升法。

## 回归树

将决策树应用于回归问题，则为回归树。对于回归问题，其响应变量y为连续变量。
故对于回归树，可使用”最小残差平方和“作为节点的分裂准则。意味着在进行节点分裂时，希望分裂后，残差平方和下降最多，即两个子节点的残差平方和之总和最小。

## C5.0
与CART算法不同，C5.0算法允许使用多叉树，而不局限于二叉树。对于分类问题，C5.0算法传统熵使用信息熵作为分裂准则，而CART算法则一般使用基尼指数作为分裂准则。在实践中，C5.0算法的预测效果与CART类似。然而，由于Breiman（2001）提出的随机森林算法，以及Friedman（2001）提出的梯度提升法均基于CART算法。
